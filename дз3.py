# -*- coding: utf-8 -*-
"""дз3_макс.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ZxPvKYRjimEecMvFc8wM5O4aMdPfN7X
"""

!pip install -U datasets sentence-transformers faiss-cpu

from datasets import load_dataset, DatasetDict
from sentence_transformers import SentenceTransformer, util
import faiss
import numpy as np

full_dataset = load_dataset("sentence-transformers/natural-questions", split="train")

# Разбиваем 80% / 20%
split = full_dataset.train_test_split(test_size=0.2, seed=42)
dataset = DatasetDict({'train': split['train'], 'test': split['test']})

model = SentenceTransformer('all-MiniLM-L6-v2')

corpus = [item['answer'] for item in dataset['train']][:5000]
corpus_embeddings = model.encode(corpus, convert_to_numpy=True, show_progress_bar=True)

index = faiss.IndexFlatL2(corpus_embeddings.shape[1])  # L2 расстояние
index.add(corpus_embeddings)

questions = [item['query'] for item in dataset['test']]
questions = questions[:5000]
question_embeddings = model.encode(questions, convert_to_numpy=True, show_progress_bar=True)

true_answers = [item['answer'] for item in dataset['test']]

true_answers = true_answers[:5000]

# Находим топ-K ближайших эмбеддингов
K = 10
distances, indices = index.search(question_embeddings, K)

# Преобразуем для Recall@K и MRR
target_ids = []
predicted_ids = []

for i, idxs in enumerate(indices):
    target = corpus.index(true_answers[i]) if true_answers[i] in corpus else -1
    if target == -1:
        continue
    target_ids.append(target)
    predicted_ids.append(idxs.tolist())

def recall_at_k(targets, predictions, k):
    correct = sum(1 for t, p in zip(targets, predictions) if t in p[:k])
    return correct / len(targets)

def mean_reciprocal_rank(targets, predictions):
    rr_total = 0.0
    for t, p in zip(targets, predictions):
        if t in p:
            rr_total += 1.0 / (p.index(t) + 1)
    return rr_total / len(targets)

print("Recall@5:", recall_at_k(target_ids, predicted_ids, 5))
print("MRR:", mean_reciprocal_rank(target_ids, predicted_ids))

"""Задача 2"""

from datasets import load_dataset
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
import numpy as np
from tqdm import tqdm

dataset = load_dataset("sentence-transformers/natural-questions", split="train")

questions = [item['query'] for item in dataset]
answers = [item['answer'] for item in dataset]

questions = questions[:5000]
answers = answers[:5000]

q_train, q_test, a_train, a_test = train_test_split(questions, answers, test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer()
tfidf_corpus_train = vectorizer.fit_transform(a_train)

tfidf_corpus_test = vectorizer.transform(a_test)
tfidf_questions = vectorizer.transform(q_test)

similarities = cosine_similarity(tfidf_questions, tfidf_corpus_test)

top_k_predictions = np.argsort(-similarities, axis=1)

target_ids = list(range(len(a_test)))
predicted_ids = top_k_predictions.tolist()

def recall_at_k(targets, predictions, k):
    return sum(1 for t, p in zip(targets, predictions) if t in p[:k]) / len(targets)

def mean_reciprocal_rank(targets, predictions):
    total = 0.0
    for t, p in zip(targets, predictions):
        if t in p:
            total += 1.0 / (p.index(t) + 1)
    return total / len(targets)

print("Recall@1:", recall_at_k(target_ids, predicted_ids, 1))
print("Recall@3:", recall_at_k(target_ids, predicted_ids, 3))
print("Recall@10:", recall_at_k(target_ids, predicted_ids, 10))
print("MRR:", mean_reciprocal_rank(target_ids, predicted_ids))

"""Вывод:
Метрики показывают, что TF-IDF хорошо находит частично релевантные ответы, но плохо справляется с синонимами, перефразировками и нечестких формулировках.

Задача 3
"""

!pip install -U transformers datasets scikit-learn

from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from tqdm import tqdm

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model_name = "intfloat/multilingual-e5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).to(device)

dataset = load_dataset("sentence-transformers/natural-questions", split="train")
questions = [item['query'] for item in dataset][:5000]
answers = [item['answer'] for item in dataset][:5000]

q_train, q_test, a_train, a_test = train_test_split(questions, answers, test_size=0.2, random_state=42)

def encode_texts(texts, prefix, batch_size=32):
    embeddings = []

    for i in tqdm(range(0, len(texts), batch_size)):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(
            [f"{prefix}: {text}" for text in batch_texts],
            padding=True, truncation=True, return_tensors="pt"
        ).to(device)

        with torch.no_grad():
            outputs = model(**inputs)
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.append(batch_embeddings)

    return np.vstack(embeddings)

question_embeddings = encode_texts(q_test, prefix="query")
answer_embeddings = encode_texts(a_test, prefix="passage")

similarities = cosine_similarity(question_embeddings, answer_embeddings)
top_k_predictions = np.argsort(-similarities, axis=1)

target_ids = list(range(len(a_test)))
predicted_ids = top_k_predictions.tolist()

def recall_at_k(targets, predictions, k):
    return sum(1 for t, p in zip(targets, predictions) if t in p[:k]) / len(targets)

def mean_reciprocal_rank(targets, predictions):
    total = 0.0
    for t, p in zip(targets, predictions):
        if t in p:
            total += 1.0 / (p.index(t) + 1)
    return total / len(targets)

print("Recall@1:", recall_at_k(target_ids, predicted_ids, 1))
print("Recall@3:", recall_at_k(target_ids, predicted_ids, 3))
print("Recall@10:", recall_at_k(target_ids, predicted_ids, 10))
print("MRR:", mean_reciprocal_rank(target_ids, predicted_ids))

"""Вывод:
Модель E5 работает лучше, чем TF-IDF. Она понимает смысл, а не просто ищет пересечение слов

Задача 4
"""

!pip install -q sentence-transformers datasets

from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
from datasets import load_dataset
import torch, random
import numpy as np
from sklearn.metrics import label_ranking_average_precision_score

train_data = load_dataset("nq_open", split="train[:5%]")
test_data = load_dataset("nq_open", split="validation[:2%]")

def get_pairs(data):
    pairs = []
    for item in data:
        question = item['question']
        answers = item['answer']
        if answers and answers[0]:
            pairs.append((question, answers[0]))
    return pairs

train_pairs = get_pairs(train_data)
test_pairs = get_pairs(test_data)
all_docs = [doc for _, doc in train_pairs]

contrastive_examples = []
for q, pos in train_pairs:
    neg = random.choice(all_docs)
    contrastive_examples.append(InputExample(texts=[q, pos], label=1.0))
    contrastive_examples.append(InputExample(texts=[q, neg], label=0.0))

triplet_examples = []
for q, pos in train_pairs:
    neg = random.choice(all_docs)
    triplet_examples.append(InputExample(texts=[q, pos, neg]))

import os
os.environ["WANDB_DISABLED"] = "true"

model_contrastive = SentenceTransformer("intfloat/multilingual-e5-base")
train_loader = DataLoader(contrastive_examples, shuffle=True, batch_size=32)
loss_contrastive = losses.CosineSimilarityLoss(model_contrastive)

model_contrastive.fit(
    train_objectives=[(train_loader, loss_contrastive)],
    epochs=1,
    warmup_steps=100,
    show_progress_bar=True
)

model_triplet = SentenceTransformer("intfloat/multilingual-e5-base")
triplet_loader = DataLoader(triplet_examples, shuffle=True, batch_size=32)
loss_triplet = losses.TripletLoss(model_triplet)

model_triplet.fit(
    train_objectives=[(triplet_loader, loss_triplet)],
    epochs=1,
    warmup_steps=100,
    show_progress_bar=True
)

test_questions = [q for q, _ in test_pairs]
test_documents = list(set([d for _, d in test_pairs]))  # убираем дубликаты

def get_ground_truth():
    gt = []
    for q, d in test_pairs:
        gt.append((q, d))
    return gt

ground_truth = get_ground_truth()

def evaluate_model(model, name):
    query_embeddings = model.encode(test_questions, convert_to_tensor=True, show_progress_bar=True)
    doc_embeddings = model.encode(test_documents, convert_to_tensor=True, show_progress_bar=True)

    scores = torch.matmul(query_embeddings, doc_embeddings.T)  # косинус
    rankings = scores.argsort(dim=-1, descending=True)

    recalls_at_k = {1: 0, 3: 0, 10: 0}
    mrr_total = 0

    for i, (q, true_doc) in enumerate(ground_truth):
        if q not in test_questions or true_doc not in test_documents:
            continue
        q_idx = test_questions.index(q)
        d_idx = test_documents.index(true_doc)
        ranking = rankings[q_idx].tolist()

        # Recall@K
        for k in recalls_at_k.keys():
            if d_idx in ranking[:k]:
                recalls_at_k[k] += 1

        # MRR
        if d_idx in ranking:
            rank = ranking.index(d_idx) + 1
            mrr_total += 1 / rank

    total = len(ground_truth)
    print(f"\nРезультаты для {name}:")
    for k in recalls_at_k:
        print(f"Recall@{k}: {recalls_at_k[k] / total:.4f}")
    print(f"MRR: {mrr_total / total:.4f}")

evaluate_model(model_contrastive, "Contrastive Loss")
evaluate_model(model_triplet, "Triplet Loss")

vanilla = SentenceTransformer("intfloat/multilingual-e5-base")
evaluate_model(vanilla, "Vanilla E5")

"""Вывод:
Contrastive Loss - лидер по всем метрикам.
Модель научилась ставить правильные документы ближе к началу списка.

Triplet Loss - сильно отстал, особенно в Recall@1 и MRR.

Vanilla E5 - неплохой базовый уровень, но уступил дообученной Contrastive-модели

3. Стало ли лучше в сравнении с ванильным E5? Почему?


Recall@1 поднялся с 0.375 до 0.4167,MRR  с 0.4999 до 0.5727

Дообучение на конкретном датасете помогло модели лучше ориентироваться в домене Natural Questions, где совпадения могут быть более контекстуальными

Задача 5
"""

from sentence_transformers import SentenceTransformer, InputExample, losses, models
from torch.utils.data import DataLoader
from datasets import load_dataset
from tqdm import tqdm
import random
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

model_name = "intfloat/multilingual-e5-base"
model = SentenceTransformer(model_name)

dataset = load_dataset("nq_open", split="train[:10000]")
test_dataset = load_dataset("nq_open", split="validation[:1000]")

def get_question_context_pairs(dataset):
    return [(item["question"], item["answer"][0]) for item in dataset if item["answer"]]

train_pairs = get_question_context_pairs(dataset)

all_contexts = [ctx for _, ctx in train_pairs]
ctx_embeddings = model.encode(all_contexts, convert_to_tensor=True, batch_size=64, show_progress_bar=True)

triplets = []
for q, pos in tqdm(train_pairs, desc="Формируем triplets"):
    q_emb = model.encode(q, convert_to_tensor=True)
    scores = cosine_similarity(q_emb.unsqueeze(0).cpu(), ctx_embeddings.cpu())[0]
    sorted_indices = np.argsort(-scores)
    # Ищем ближайший нерелевантный документ
    for idx in sorted_indices:
        hard_neg = all_contexts[idx]
        if hard_neg != pos:
            triplets.append(InputExample(texts=[q, pos, hard_neg]))
            break

train_dataloader = DataLoader(triplets, shuffle=True, batch_size=16)

train_loss = losses.TripletLoss(model)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=1,
    warmup_steps=100,
    show_progress_bar=True
)

test_qs = [item["question"] for item in test_dataset if item["answer"]]
test_docs = [item["answer"][0] for item in test_dataset if item["answer"]]

q_embeddings = model.encode(test_qs, convert_to_tensor=True, show_progress_bar=True)
d_embeddings = model.encode(test_docs, convert_to_tensor=True, show_progress_bar=True)

def compute_metrics(q_embeddings, d_embeddings, top_k_list=[1, 3, 10]):
    sim_matrix = cosine_similarity(q_embeddings.cpu(), d_embeddings.cpu())
    metrics = {f"Recall@{k}": 0 for k in top_k_list}
    mrr = 0.0
    total = len(q_embeddings)

    for i in range(total):
        sim_scores = sim_matrix[i]
        sorted_idx = np.argsort(-sim_scores)

        rank = np.where(sorted_idx == i)[0][0] + 1  # +1 because ranks start at 1
        mrr += 1 / rank

        for k in top_k_list:
            if rank <= k:
                metrics[f"Recall@{k}"] += 1

    for k in top_k_list:
        metrics[f"Recall@{k}"] /= total
    metrics["MRR"] = mrr / total
    return metrics

results = compute_metrics(q_embeddings, d_embeddings)
for k, v in results.items():
    print(f"{k}: {v:.4f}")

"""Результаты получились намного хуже."""